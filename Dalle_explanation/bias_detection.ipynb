{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection bias based on bias token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update code with cliff $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def cliff_delta(x: List[float], y: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Cliff's Delta.\n",
    "    x, y: Python lists or NumPy arrays of numeric values.\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    Nx = len(x)\n",
    "    Ny = len(y)\n",
    "    total = 0.0\n",
    "    for i in range(Nx):\n",
    "        for j in range(Ny):\n",
    "            if x[i] > y[j]:\n",
    "                total += 1\n",
    "            elif x[i] == y[j]:\n",
    "                total += 0.5\n",
    "    return (total - (Nx * Ny) / 2) / (Nx * Ny)\n",
    "\n",
    "def load_top_n_tokens(csv_path: str, top_n: int, token_num: int) -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Load multiple tokens from the Top N section of the specified CSV file.\n",
    "    \"\"\"\n",
    "    tokens_data = []\n",
    "    with open(csv_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        in_top_n_section = False\n",
    "        current_row = 0\n",
    "        \n",
    "        for row in reader:\n",
    "            if f\"Top {top_n} Tokens\" in row:\n",
    "                in_top_n_section = True\n",
    "                continue\n",
    "            \n",
    "            if in_top_n_section and row and row[0] == \"Token\":\n",
    "                continue\n",
    "            \n",
    "            if in_top_n_section and row:\n",
    "                if current_row < token_num:\n",
    "                    token = row[0]\n",
    "                    files = row[2].split('; ')\n",
    "                    tokens_data.append((token, files))\n",
    "                    current_row += 1\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return tokens_data\n",
    "\n",
    "def get_token_frequencies(token_list: List[int], data_csv_path: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get the total frequency of all tokens (as a group) in each file.\n",
    "    \"\"\"\n",
    "    frequencies = []\n",
    "    \n",
    "    with open(data_csv_path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            indices = ast.literal_eval(row['indices'])\n",
    "            # Count how many times any token from token_list appears in this file\n",
    "            token_count = sum(1 for idx in indices if idx in token_list)\n",
    "            frequencies.append(token_count)\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "def perform_cliff(freqs1: List[float], freqs2: List[float]) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform Mann-Whitney U test between two groups of frequencies,\n",
    "    and also compute Cliff's Delta as an effect size measure.\n",
    "    \"\"\"\n",
    "    cd = cliff_delta(freqs1, freqs2)\n",
    "    \n",
    "    return {\n",
    "        'cliff_delta': cd\n",
    "    }\n",
    "\n",
    "def analyze_token_statistics(tokens_data: List[Tuple[str, List[str]]], data_csv_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze statistics for the specified tokens in the dataset.\n",
    "    \"\"\"\n",
    "    token_list = [int(token) for token, _ in tokens_data]\n",
    "    \n",
    "    # Get frequencies for this group of tokens\n",
    "    frequencies = get_token_frequencies(token_list, data_csv_path)\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    total_files = len(frequencies)\n",
    "    total_token_occurrences = sum(frequencies)\n",
    "    \n",
    "    stats = {\n",
    "        'total_files': total_files,\n",
    "        'total_token_occurrences': total_token_occurrences,\n",
    "        'average_frequency_per_file': (total_token_occurrences / total_files) if total_files > 0 else 0,\n",
    "        'number_of_target_tokens': len(token_list),\n",
    "        'frequencies': frequencies \n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def process_all_labels(input_folder: str, data_csv_path: str, output_folder: str,\n",
    "                       top_n: int = 1, token_num: int = 50):\n",
    "    \"\"\"\n",
    "    Process all label CSV files and save combined results.\n",
    "    \"\"\"\n",
    "    # Create output folder for bias detection results\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Store frequencies for Mann-Whitney U test\n",
    "    label_frequencies = {}\n",
    "    \n",
    "    # Process each label file\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.startswith('label_') and filename.endswith('.csv'):\n",
    "            # Extract label number\n",
    "            label_num = re.search(r'label_(\\d+)\\.csv', filename)\n",
    "            if not label_num:\n",
    "                continue\n",
    "            \n",
    "            label_num = label_num.group(1)\n",
    "            print(f\"\\nProcessing label {label_num}...\")\n",
    "            \n",
    "            # Load and analyze tokens\n",
    "            csv_path = os.path.join(input_folder, filename)\n",
    "            tokens_data = load_top_n_tokens(csv_path, top_n, token_num)\n",
    "            \n",
    "            if not tokens_data:\n",
    "                print(f\"Failed to load tokens for label {label_num}\")\n",
    "                continue\n",
    "            \n",
    "            # Get statistics\n",
    "            stats_dict = analyze_token_statistics(tokens_data, data_csv_path)\n",
    "            stats_dict['label'] = label_num\n",
    "            stats_dict['tokens'] = [token for token, _ in tokens_data]\n",
    "            \n",
    "            label_frequencies[label_num] = stats_dict['frequencies']\n",
    "            del stats_dict['frequencies']\n",
    "            \n",
    "            all_results.append(stats_dict)\n",
    "    \n",
    "    if len(label_frequencies) >= 2:\n",
    "        labels = sorted(label_frequencies.keys())\n",
    "        for result in all_results:\n",
    "            label1 = result['label']\n",
    "            label2 = labels[1] if label1 == labels[0] else labels[0]\n",
    "            # print(label1, label2) \n",
    "            test_results = perform_cliff(\n",
    "                label_frequencies[label1],\n",
    "                label_frequencies[label2]\n",
    "            )\n",
    "            \n",
    "            # Add test results (including Cliff's Delta) to stats\n",
    "            result['cliff_delta'] = test_results['cliff_delta']\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nCliff delta results for label {label1}:\")\n",
    "            print(f\"  Cliff's Delta:{test_results['cliff_delta']:.4f}\")\n",
    "    \n",
    "    # Save combined results\n",
    "    if all_results:\n",
    "        combined_df = pd.DataFrame(all_results)\n",
    "        output_filename = f'token_bias_top{top_n}_num{token_num}_cliff.csv'\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "    \n",
    "        if 'cliff_delta' in combined_df.columns:\n",
    "            combined_df['cliff_delta'] = combined_df['cliff_delta'].apply(lambda x: f'{x:.4f}')\n",
    "        \n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nSaved combined statistics (including Cliff's Delta) to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    bias_type = \"doctor_color\"\n",
    "    model_number = 1\n",
    "    # Configuration\n",
    "    input_folder = f\"results/{bias_type}/TIS/Net{model_number}/TIS_statistics_train\"\n",
    "    data_csv_path = f\"datasets/doctor_original/information/results_none.csv\"\n",
    "    output_folder = f\"results/{bias_type}/Bias_detection/\"\n",
    "    \n",
    "    # Parameters\n",
    "    top_n = 5\n",
    "    token_num = 10\n",
    "    \n",
    "    # Process all labels\n",
    "    process_all_labels(input_folder, data_csv_path, output_folder, top_n, token_num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
